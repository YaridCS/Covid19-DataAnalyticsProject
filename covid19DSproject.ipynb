{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covid-19 Computational Data Analytics and Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to implement the data into our code. \n",
    "\n",
    "We can do this by using the requests.get function \n",
    "and reading the data contained in a .csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv'\n",
    "response = requests.get(url)\n",
    "data = io.StringIO(response.content.decode('utf-8'))\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "\n",
    "df.to_csv('covid19_datatable_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an image of what our dataset currently looks like. \n",
    "\n",
    "\n",
    "![Image not loaded](https://i.imgur.com/fidBhWp.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we can start organizing and cleaning the dataset so it is easier to read and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating data from the columns of interest\n",
    "df = df[['location', 'total_cases', 'total_deaths', 'total_tests', 'positive_rate', \n",
    "'total_vaccinations', 'people_vaccinated', 'total_boosters', 'people_fully_vaccinated']]\n",
    "\n",
    "\n",
    "# Renaming the columns for better readability\n",
    "df.columns = ['Country or Location', 'Cumulative Total of Confirmed Covid-19 Cases', 'Cumulative Total of Confirmed Covid-19 Deaths', \n",
    "'Total Number of Covid-19 Tests Taken', 'Share of Positive Covid-19 Tests (%)', \n",
    "'Total Number of Vaccinations Administered', 'Total Number of People who Received at Least One Vaccine Dose', \n",
    "'Total Number of People who Received Only the Booster Dose', 'Total Number of People Who Receieved All Vacine Doses']\n",
    "\n",
    "\n",
    "# Replace the duplicate entries by taking the maximum value of each column for each Country or Location\n",
    "\n",
    "df = df.groupby('Country or Location').agg({\n",
    "    'Cumulative Total of Confirmed Covid-19 Cases': 'max', \n",
    "    'Cumulative Total of Confirmed Covid-19 Deaths': 'max', \n",
    "    'Total Number of Covid-19 Tests Taken': 'max', \n",
    "    'Share of Positive Covid-19 Tests (%)': 'max', \n",
    "    'Total Number of Vaccinations Administered': 'max', \n",
    "    'Total Number of People who Received at Least One Vaccine Dose': 'max',\n",
    "    'Total Number of People who Received Only the Booster Dose': 'max',\n",
    "    'Total Number of People Who Receieved All Vacine Doses': 'max',\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "# Fill in missing values\n",
    "\n",
    "df.fillna(\"Data Not Observed\", inplace=True)\n",
    "\n",
    "\n",
    "df.to_csv('covid19_datatable_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns have been renamed for better understandability and clarity. The missing data entries have been filled in \n",
    "with \"Data Not Observed.\" \n",
    "\n",
    "Duplicate entries for the same country or location have also been removed. The dataset should now be much easier to read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/0uchf5o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/49Nfp4Z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this column, we can see that I wanted the data to be measured in percents for clarity and readibility reasons. However, the data stored here\n",
    "was not correctly updated to reflect a proper percentage so it needs to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round2percent(value):\n",
    "    if value == \"Data Not Observed\":\n",
    "        return value\n",
    "    else:\n",
    "        return round(value * 100, 2)\n",
    "\n",
    "df['Share of Positive Covid-19 Tests (%)'] = df['Share of Positive Covid-19 Tests (%)'].apply(round2percent)\n",
    "\n",
    "\n",
    "df.to_csv('covid19_datatable_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/wX0GUaP.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, the numbers can range from being extremely large to being extremely small. \n",
    "\n",
    "This means it can be difficult to properly read and understand what each data entry \n",
    "means and its overall value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the entry is above 1 million, I want to round to the nearest million and then to the nearest tenth. So for\n",
    "# example an entry should be 125.5 million. \n",
    "\n",
    "def millions_rounding(value_entry):\n",
    "    if value_entry == \"Data Not Observed\":\n",
    "        return value_entry\n",
    "    value_entry = float(value_entry)\n",
    "    if value_entry >= 1_000_000:\n",
    "        return f\"{value_entry / 1_000_000:.1f} million\".replace(\".0 million\", \" million\")\n",
    "    else:\n",
    "        return f\"{value_entry:,.0f}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Leave out the Positive Tests column since the data in that column is in percents\n",
    "\n",
    "columns_to_change = [\n",
    "    'Cumulative Total of Confirmed Covid-19 Cases', \n",
    "    'Cumulative Total of Confirmed Covid-19 Deaths', \n",
    "    'Total Number of Covid-19 Tests Taken', \n",
    "    'Total Number of Vaccinations Administered', \n",
    "    'Total Number of People who Received at Least One Vaccine Dose', \n",
    "    'Total Number of People who Received Only the Booster Dose',\n",
    "    'Total Number of People Who Receieved All Vacine Doses'\n",
    "]\n",
    "\n",
    "for entry in columns_to_change:\n",
    "    df[entry] = df[entry].apply(millions_rounding)\n",
    "\n",
    "\n",
    "df.to_csv('covid19_datatable_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/4457JDf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data clustering can be performed in large data sets like these to see observable trends within the data.\n",
    "The only problem is that there are quite a few missing data entries. One solution is to implement a prediction algorithm that can help estimate the values of these missing data entries. However, by doing this, the authenticity of \n",
    "the dataset will be ruined since we will be using made up values instead of real, observed data. For this reason, we can create multiple datasets with different prediction algorithms and observe the difference in clustering results when compared to the original data set. The original and unedited dataset will still be present, but with prediction algorithms to fill in the data, perhaps more conclusions can be drawn from the data observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# We need to replace the strings with integers to perform math operations\n",
    "\n",
    "df.replace(\"Data Not Observed\", np.nan, inplace=True)\n",
    "\n",
    "columns_to_use = [\n",
    "    'Cumulative Total of Confirmed Covid-19 Cases', \n",
    "    'Cumulative Total of Confirmed Covid-19 Deaths', \n",
    "    'Total Number of Covid-19 Tests Taken', \n",
    "    'Total Number of Vaccinations Administered', \n",
    "    'Total Number of People who Received at Least One Vaccine Dose', \n",
    "    'Total Number of People who Received Only the Booster Dose',\n",
    "    'Total Number of People Who Receieved All Vacine Doses'\n",
    "]\n",
    "\n",
    "# We need to revert back to our regular numbers we used for the dataset instead of the way we rounded to the \n",
    "# nearest million\n",
    "\n",
    "def undo_millions_rounding(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        if \"million\" in value:\n",
    "            return float(value.replace(\" million\", \"\")) * 1_000_000\n",
    "        elif \",\" in value:\n",
    "            return float(value.replace(\",\", \"\"))\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "for entry in columns_to_use:\n",
    "    df[entry] = df[entry].apply(undo_millions_rounding)\n",
    "\n",
    "# We can store our original value for the country or location column in another variable and delete\n",
    "# it from the dataset temporarily to perform our algorithm \n",
    "\n",
    "new_countries = df['Country or Location']\n",
    "dropped_column_DS = df.drop('Country or Location', axis=1)\n",
    "\n",
    "# Now we can implement the KNN algorithm\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5) # In this example, our K value is 5, we can change this to any number\n",
    "#                                         number of neighbors to run the KNN algorithm on\n",
    "\n",
    "knn_imputeDS = knn_imputer.fit_transform(dropped_column_DS)\n",
    "knn_imputeDF = pd.DataFrame(knn_imputeDS, columns=dropped_column_DS.columns)\n",
    "knn_imputeDF['Country or Location'] = new_countries\n",
    "\n",
    "# If the country or location had no neighbors to use in the KNN computation, we can fill them using the median of the columns\n",
    "knn_imputeDF[columns_to_use] = knn_imputeDF[columns_to_use].fillna(knn_imputeDF[columns_to_use].median())\n",
    "\n",
    "# Now we need to reupdate the ordering of columns and make sure our Country column is the first one\n",
    "\n",
    "col = ['Country or Location'] + [col for col in knn_imputeDF.columns if col != 'Country or Location']\n",
    "knn_imputeDF = knn_imputeDF[col]\n",
    "\n",
    "knn_imputeDF.to_csv('KNN_covid_data_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/mJ2RHPy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN algorithm has finally been implemented to fill in the missing data entries. However, the dataset\n",
    "has become messy again and difficult to read. It needs to be updated using the same methods we used previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('KNN_covid_data_table.csv')\n",
    "df.replace(\"Data Not Observed\", np.nan, inplace=True)\n",
    "\n",
    "# Make sure entries in 'Share of Positive Covid-19 Tests (%)' column are rounded to the nearest hundredth\n",
    "df['Share of Positive Covid-19 Tests (%)'] = df['Share of Positive Covid-19 Tests (%)'].round(2)\n",
    "\n",
    "\n",
    "df.fillna(\"Data Not Observed\", inplace=True)\n",
    "\n",
    "# Apply the millions_rounding function to each data entry in every column \n",
    "\n",
    "for col in columns_to_use:\n",
    "    df[col] = df[col].apply(millions_rounding)\n",
    "\n",
    "\n",
    "df.to_csv('KNN_covid_data_table.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image not loaded](https://i.imgur.com/lxWeSwZ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "# We use a module called os and join paths with the directory to load up our .csv files. The reason I did this \n",
    "# is so that anyone with the saved .csv files from running my code is able to perform the actions of this project.\n",
    "# In other words, I did not want to load the files locally so the code is inaccessible for other people to run.\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "dataset1 = pd.read_csv(os.path.join(current_directory, 'covid19_datatable_cleaned.csv'))\n",
    "dataset2 = pd.read_csv(os.path.join(current_directory, 'KNN_covid_data_table.csv'))\n",
    "\n",
    "\n",
    "\n",
    "# Now we can begin the clustering part. In this case, I used the DBSCAN algorithm to perform the dataset clustering\n",
    "\n",
    "def updating_data_entries(df):\n",
    "\n",
    "    # We need to drop the country or location column again since it is not numeric\n",
    "\n",
    "    numeric_df = df.drop('Country or Location', axis=1)\n",
    "    \n",
    "    # We need to replace all the strings with an integer type\n",
    "\n",
    "    numeric_df.replace(\"Data Not Observed\", np.nan, inplace=True)\n",
    "\n",
    "    # We have to undo the millions rounding function that we used on our data \n",
    "\n",
    "    for column in columns_to_use:\n",
    "        numeric_df[column] = numeric_df[column].apply(undo_millions_rounding)\n",
    "\n",
    "    # Now we can covert all data to a numeric data frame without issue\n",
    "\n",
    "    numeric_df = numeric_df.apply(pd.to_numeric)\n",
    "\n",
    "    # For values that are still missing we can use the median values of that respective column to fill it in\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputed_data = imputer.fit_transform(numeric_df)\n",
    "\n",
    "    # We need to make sure the data is scaled correctly and properly as we make the dataset\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(imputed_data)\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "# Now we can run the function on our datasets\n",
    "\n",
    "updated_dataset1 = updating_data_entries(dataset1)\n",
    "updated_dataset2 = updating_data_entries(dataset2)\n",
    "\n",
    "# We need to define the epsilon distance (max distance between two neighboring points) and min samples\n",
    "# (the minimum number of samples required for a neighbor of data entries to be considered a cluster) I used 3 and 2\n",
    "# but this can be changed\n",
    "\n",
    "eps = 3\n",
    "min_samples = 2\n",
    "\n",
    "# Finally we can correctly run the DBSCAN algorithm on our data sets using the proper formatting\n",
    "\n",
    "DBSCAN_set1 = DBSCAN(eps=eps, min_samples=min_samples).fit(updated_dataset1)\n",
    "DBSCAN_set2 = DBSCAN(eps=eps, min_samples=min_samples).fit(updated_dataset2)\n",
    "\n",
    "\n",
    "# Reassess our data for cluster in each dataset\n",
    "\n",
    "dataset1['Cluster'] = DBSCAN_set1.labels_\n",
    "dataset2['Cluster'] = DBSCAN_set2.labels_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the clustering is finished, the visualization section needs to be coded so it can be seen and analyzed.\n",
    "\n",
    "For this project I used the DBSCAN clustering algorithm. The reason for this was because this clustering algorithm\n",
    "is able to resist outliers and large variances in data. It is also able to handle and accurately cluster large\n",
    "amounts of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We need to select a wide range of colors to represent the number of clusters\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', \n",
    "          '#f57c00', '#3f51b5', '#008080', '#006400', '#ffd700', \n",
    "          '#1e90ff', '#9932cc', '#ff1493', '#dc143c']\n",
    "\n",
    "\n",
    "sorted_data = dataset1.sort_values(by=['Cumulative Total of Confirmed Covid-19 Cases', 'Cumulative Total of Confirmed Covid-19 Deaths'], ascending=True)\n",
    "\n",
    "# Adding a new column of data for clusters\n",
    "\n",
    "sorted_data['ClusterCount'] = sorted_data.groupby('Cluster')['Cluster'].transform('count')\n",
    "\n",
    "# Make the scatter plot, x is number of cases, y is number of deaths\n",
    "\n",
    "fig = px.scatter(sorted_data, \n",
    "                 x='Cumulative Total of Confirmed Covid-19 Cases', \n",
    "                 y='Cumulative Total of Confirmed Covid-19 Deaths', \n",
    "                 color='Cluster', \n",
    "                 title='COVID-19 Clusters by Country', \n",
    "                 color_discrete_sequence=colors,\n",
    "                 hover_name='Country or Location')\n",
    "\n",
    "\n",
    "# Fixing up and resizing the scatter plot\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"COVID-19 Clusters by Country (Dataset 1)\",\n",
    "        'x': 0.5,\n",
    "        'y': 0.95,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title='Total Confirmed COVID-19 Cases', \n",
    "    yaxis_title='Total Confirmed COVID-19 Deaths',\n",
    "    title_x=0.5)\n",
    "\n",
    "fig.update_traces(marker_size=10, marker_opacity=0.7)\n",
    "fig.write_html(\"covid19original_DS_cluster.html\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://nbviewer.org/github/YaridCS/Covid19-DataAnalyticsProject/blob/c846b7e01b7ac49e74618e041d2e42505a51d5da/covid19original_DS_cluster.html) to view the cluster scatter plot from the `covid19original_DS_cluster.html` file for Dataset1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "    \n",
    "Now we can do the same for dataset 2 which has our prediction algorithms implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# We need to select a wide range of colors to represent the number of clusters\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \n",
    "          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', \n",
    "          '#f57c00', '#3f51b5', '#008080', '#006400', '#ffd700', \n",
    "          '#1e90ff', '#9932cc', '#ff1493', '#dc143c']\n",
    "\n",
    "# Sort the data so it is easier to work with alter on \n",
    "\n",
    "sorted_data = dataset2.sort_values(by=['Cumulative Total of Confirmed Covid-19 Cases', 'Cumulative Total of Confirmed Covid-19 Deaths'], ascending=True)\n",
    "\n",
    "# Add a column of data for the number of clusters\n",
    "\n",
    "sorted_data['ClusterCount'] = sorted_data.groupby('Cluster')['Cluster'].transform('count')\n",
    "\n",
    "# Make the scatter plot, x is number of cases, y is number of deaths\n",
    "\n",
    "fig = px.scatter(sorted_data, \n",
    "                 x='Cumulative Total of Confirmed Covid-19 Cases', \n",
    "                 y='Cumulative Total of Confirmed Covid-19 Deaths', \n",
    "                 color='Cluster', \n",
    "                 title='COVID-19 Clusters by Country (Dataset 2)', \n",
    "                 color_discrete_sequence=colors,\n",
    "                 hover_name='Country or Location')\n",
    "\n",
    "\n",
    "# Fixing up and resizing the scatter plot\n",
    "\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"COVID-19 Clusters by Country (Dataset 2)\",\n",
    "        'x': 0.5,\n",
    "        'y': 0.95,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    xaxis_title='Total Confirmed COVID-19 Cases', \n",
    "    yaxis_title='Total Confirmed COVID-19 Deaths',\n",
    "    title_x=0.5)\n",
    "\n",
    "fig.update_traces(marker_size=10, marker_opacity=0.7)\n",
    "\n",
    "\n",
    "fig.write_html(\"covid19_predictional_algorithm_cluster.html\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://nbviewer.org/github/YaridCS/Covid19-DataAnalyticsProject/blob/284cd8a2fd2e04ad1dc75106d3f82d497f67d261/covid19_predictional_algorithm_cluster.html) to view the cluster scatter plot from the `covid19_predictional_algorithm_cluster.html` file for Dataset2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
